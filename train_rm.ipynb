{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1462f8-b9f6-40d8-8374-bd70630a6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043e2ad4-2da4-44a2-a2f3-6ee0bf6bd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/scratch/users/jiaxun1218\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-7B\"  # adjust if your HF ID differs\n",
    "MAX_LEN = 1024  # prompt + answer snippet; tune as needed\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 3\n",
    "LR = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "label2id = {\n",
    "    \"model_a\": 0,\n",
    "    \"model_b\": 1,\n",
    "    \"tie\": 2,\n",
    "    \"both_bad\": 3,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e241a3cb-2536-429b-9a6b-8682a0723e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseArenaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects a list of dicts, each with at least:\n",
    "      {\n",
    "        \"question\": str,\n",
    "        \"answer_a\": str,\n",
    "        \"answer_b\": str,\n",
    "        \"human_label\": \"model_a\" | \"model_b\" | \"tie\" | \"both_bad\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[Dict], tokenizer: AutoTokenizer, max_len: int = 1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.system_prefix = (\n",
    "            \"You are a strict math answer judge. \"\n",
    "            \"You will be given a question and an answer. \"\n",
    "            \"Evaluate the answer's correctness and reasoning quality.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    def build_text(self, question: str, answer: str) -> str:\n",
    "        return (\n",
    "            self.system_prefix\n",
    "            + \"Question:\\n\"\n",
    "            + question.strip()\n",
    "            + \"\\n\\nAnswer:\\n\"\n",
    "            + answer.strip()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        q = item[\"question\"]\n",
    "        aA = item[\"answer_a\"]\n",
    "        aB = item[\"answer_b\"]\n",
    "        lab_str = item[\"human_label\"]\n",
    "\n",
    "        # map label string -> int id\n",
    "        label_id = label2id[lab_str]\n",
    "\n",
    "        text_a = self.build_text(q, aA)\n",
    "        text_b = self.build_text(q, aB)\n",
    "\n",
    "        enc_a = self.tokenizer(\n",
    "            text_a,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc_b = self.tokenizer(\n",
    "            text_b,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_a\": enc_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": enc_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": enc_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": enc_b[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label_id, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Collator\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class PairwiseCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    pad_to_multiple_of: int = 8\n",
    "\n",
    "    def __call__(self, batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # We need to pad A and B separately\n",
    "        ids_a = [x[\"input_ids_a\"] for x in batch]\n",
    "        mask_a = [x[\"attention_mask_a\"] for x in batch]\n",
    "        ids_b = [x[\"input_ids_b\"] for x in batch]\n",
    "        mask_b = [x[\"attention_mask_b\"] for x in batch]\n",
    "        labels = torch.stack([x[\"labels\"] for x in batch], dim=0)\n",
    "\n",
    "        enc_a = self.tokenizer.pad(\n",
    "            {\"input_ids\": ids_a, \"attention_mask\": mask_a},\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        enc_b = self.tokenizer.pad(\n",
    "            {\"input_ids\": ids_b, \"attention_mask\": mask_b},\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids_a\": enc_a[\"input_ids\"],\n",
    "            \"attention_mask_a\": enc_a[\"attention_mask\"],\n",
    "            \"input_ids_b\": enc_b[\"input_ids\"],\n",
    "            \"attention_mask_b\": enc_b[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d5d5df-56de-4bda-895b-a1c6d00f2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_loss(\n",
    "    scores_a: torch.Tensor,\n",
    "    scores_b: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    lambda_pair: float = 1.0,\n",
    "    lambda_unary: float = 0.2,\n",
    "    w_loser: float = 0.5,\n",
    "    tau: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    labels: 0 = model_a, 1 = model_b, 2 = tie, 3 = both_bad\n",
    "    \"\"\"\n",
    "    sA = scores_a\n",
    "    sB = scores_b\n",
    "    d = sA - sB\n",
    "\n",
    "    L_pair = torch.zeros_like(d)\n",
    "    L_unary = torch.zeros_like(d)\n",
    "\n",
    "    # A wins\n",
    "    mask_a = (labels == 0)\n",
    "    if mask_a.any():\n",
    "        d_a = d[mask_a]\n",
    "        L_pair[mask_a] = -torch.log(torch.sigmoid(d_a) + 1e-8)\n",
    "\n",
    "        sA_a = sA[mask_a]\n",
    "        sB_a = sB[mask_a]\n",
    "        L_unary[mask_a] = (\n",
    "            F.binary_cross_entropy_with_logits(\n",
    "                sA_a, torch.ones_like(sA_a), reduction=\"none\"\n",
    "            )\n",
    "            + w_loser * F.binary_cross_entropy_with_logits(\n",
    "                sB_a, torch.zeros_like(sB_a), reduction=\"none\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # B wins\n",
    "    mask_b = (labels == 1)\n",
    "    if mask_b.any():\n",
    "        d_b = d[mask_b]\n",
    "        L_pair[mask_b] = -torch.log(torch.sigmoid(-d_b) + 1e-8)\n",
    "\n",
    "        sA_b = sA[mask_b]\n",
    "        sB_b = sB[mask_b]\n",
    "        L_unary[mask_b] = (\n",
    "            w_loser * F.binary_cross_entropy_with_logits(\n",
    "                sA_b, torch.zeros_like(sA_b), reduction=\"none\"\n",
    "            )\n",
    "            + F.binary_cross_entropy_with_logits(\n",
    "                sB_b, torch.ones_like(sB_b), reduction=\"none\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Tie\n",
    "    mask_tie = (labels == 2)\n",
    "    if mask_tie.any():\n",
    "        d_t = d[mask_tie]\n",
    "        L_pair[mask_tie] = (d_t ** 2) / (tau ** 2)\n",
    "        L_unary[mask_tie] = 0.0\n",
    "\n",
    "    # Both bad\n",
    "    mask_bad = (labels == 3)\n",
    "    if mask_bad.any():\n",
    "        sA_bad = sA[mask_bad]\n",
    "        sB_bad = sB[mask_bad]\n",
    "        L_pair[mask_bad] = 0.0\n",
    "        L_unary[mask_bad] = (\n",
    "            F.binary_cross_entropy_with_logits(\n",
    "                sA_bad, torch.zeros_like(sA_bad), reduction=\"none\"\n",
    "            )\n",
    "            + F.binary_cross_entropy_with_logits(\n",
    "                sB_bad, torch.zeros_like(sB_bad), reduction=\"none\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    loss = lambda_pair * L_pair + lambda_unary * L_unary\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. RewardModel wrapper\n",
    "# -------------------------\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a causal LM with a scalar reward head.\n",
    "    LoRA is applied to the LM via PEFT; the reward_head is a normal nn.Linear.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model: AutoModelForCausalLM):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "\n",
    "        # Get base model dtype (bf16 in your case)\n",
    "        base_dtype = next(base_model.parameters()).dtype\n",
    "\n",
    "        # Make reward head use same dtype\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "        self.reward_head.to(dtype=base_dtype)\n",
    "\n",
    "    def encode(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]  # (batch, seq, hidden)\n",
    "\n",
    "        last_indices = attention_mask.sum(dim=1) - 1\n",
    "        batch_idx = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
    "        last_hidden = hidden_states[batch_idx, last_indices]  # (batch, hidden)\n",
    "\n",
    "        # last_hidden is bf16, reward_head weights are bf16 â†’ OK\n",
    "        scores = self.reward_head(last_hidden).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_a: torch.Tensor,\n",
    "        attention_mask_a: torch.Tensor,\n",
    "        input_ids_b: torch.Tensor,\n",
    "        attention_mask_b: torch.Tensor,\n",
    "        labels: torch.Tensor = None,\n",
    "    ):\n",
    "        sA = self.encode(input_ids_a, attention_mask_a)\n",
    "        sB = self.encode(input_ids_b, attention_mask_b)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = reward_loss(sA, sB, labels)\n",
    "            return {\"loss\": loss, \"scores_a\": sA, \"scores_b\": sB}\n",
    "        else:\n",
    "            return {\"scores_a\": sA, \"scores_b\": sB}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Build model + LoRA\n",
    "# -------------------------\n",
    "\n",
    "def build_model_with_lora():\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    config.output_hidden_states = True  # so we can access hidden states\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": DEVICE},\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "\n",
    "    # LoRA config: adjust target_modules as needed for Qwen2.5\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    )\n",
    "    base_model = get_peft_model(base_model, lora_config)\n",
    "    base_model.print_trainable_parameters()  # sanity check\n",
    "\n",
    "    rm = RewardModel(base_model)\n",
    "    rm.to(DEVICE)\n",
    "    return rm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d92a12fe-3c58-409b-b2bc-542310292350",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 0.2   # tie margin\n",
    "TAU   = -1.0  # both_bad threshold on scores\n",
    "\n",
    "def train_reward_model(\n",
    "    train_data: List[Dict],\n",
    "    val_data: List[Dict] = None,   # optional, not used below but kept for future\n",
    "    test_data: List[Dict] = None,\n",
    "    delta: float = DELTA,\n",
    "    tau: float = TAU,\n",
    "):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_ds = PairwiseArenaDataset(train_data, tokenizer, max_len=MAX_LEN)\n",
    "    collator = PairwiseCollator(tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "\n",
    "    # Build model with LoRA\n",
    "    model = build_model_with_lora()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Optional test loader\n",
    "    if test_data is not None:\n",
    "        test_ds = PairwiseArenaDataset(test_data, tokenizer, max_len=MAX_LEN)\n",
    "        test_loader = DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=1,          # easier for per-example stats\n",
    "            shuffle=False,\n",
    "            collate_fn=collator,\n",
    "        )\n",
    "    else:\n",
    "        test_loader = None\n",
    "\n",
    "    # --------------------------\n",
    "    # Training loop\n",
    "    # --------------------------\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            out = model(\n",
    "                input_ids_a=batch[\"input_ids_a\"],\n",
    "                attention_mask_a=batch[\"attention_mask_a\"],\n",
    "                input_ids_b=batch[\"input_ids_b\"],\n",
    "                attention_mask_b=batch[\"attention_mask_b\"],\n",
    "                labels=batch[\"labels\"],\n",
    "            )\n",
    "            loss = out[\"loss\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Step {step+1} | Train Loss {total_loss/(step+1):.4f}\")\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Avg Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # --------------------------\n",
    "        #   Test-set evaluation\n",
    "        # --------------------------\n",
    "        if test_loader is not None:\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            per_label_counts = Counter()\n",
    "            per_label_correct = Counter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "                    out = model(\n",
    "                        input_ids_a=batch[\"input_ids_a\"],\n",
    "                        attention_mask_a=batch[\"attention_mask_a\"],\n",
    "                        input_ids_b=batch[\"input_ids_b\"],\n",
    "                        attention_mask_b=batch[\"attention_mask_b\"],\n",
    "                        labels=batch[\"labels\"],\n",
    "                    )\n",
    "\n",
    "                    loss = out[\"loss\"]\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    sA = out[\"scores_a\"]  # shape (1,)\n",
    "                    sB = out[\"scores_b\"]  # shape (1,)\n",
    "                    labels_tensor = batch[\"labels\"]  # shape (1,)\n",
    "\n",
    "                    # Convert to 1D scalars\n",
    "                    sA_val = sA.item()\n",
    "                    sB_val = sB.item()\n",
    "                    gold_id = labels_tensor.item()\n",
    "\n",
    "                    # ---- 4-way prediction using delta & tau ----\n",
    "                    d = sA_val - sB_val\n",
    "\n",
    "                    if (sA_val < tau) and (sB_val < tau):\n",
    "                        pred_label = \"both_bad\"\n",
    "                    elif d > delta:\n",
    "                        pred_label = \"model_a\"\n",
    "                    elif d < -delta:\n",
    "                        pred_label = \"model_b\"\n",
    "                    else:\n",
    "                        pred_label = \"tie\"\n",
    "\n",
    "                    pred_id = label2id[pred_label]\n",
    "\n",
    "                    total += 1\n",
    "                    per_label_counts[id2label[gold_id]] += 1\n",
    "\n",
    "                    if pred_id == gold_id:\n",
    "                        correct += 1\n",
    "                        per_label_correct[id2label[gold_id]] += 1\n",
    "\n",
    "            overall_acc = correct / total if total > 0 else 0.0\n",
    "            avg_test_loss = test_loss / total if total > 0 else 0.0\n",
    "\n",
    "            print(f\"[Epoch {epoch+1}] Test Avg Loss = {avg_test_loss:.4f} | Overall Test Acc = {overall_acc:.4f}\")\n",
    "            print(\"  Per-label accuracy:\")\n",
    "            for lab, cnt in per_label_counts.items():\n",
    "                acc_lab = per_label_correct[lab] / cnt if cnt > 0 else 0.0\n",
    "                print(f\"    {lab:9s}: {acc_lab:.4f} (n={cnt})\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Save trained LoRA RM\n",
    "    # --------------------------\n",
    "    save_dir = \"./models/qwen2_5_math7b_reward_lora_bs_1\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.base_model.save_pretrained(save_dir)\n",
    "    torch.save(model.reward_head.state_dict(), os.path.join(save_dir, \"reward_head.pt\"))\n",
    "    print(f\"Saved LoRA RM to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a086bf-5a99-4c61-8484-98f6a81532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_arena_json(path: str):\n",
    "    \"\"\"\n",
    "    Expects a JSON file containing a list of dicts like:\n",
    "\n",
    "    [\n",
    "      {\n",
    "        \"id\": \"...\",\n",
    "        \"question\": \"...\",\n",
    "        \"model_a\": \"...\",\n",
    "        \"model_b\": \"...\",\n",
    "        \"answer_a\": \"...\",\n",
    "        \"answer_b\": \"...\",\n",
    "        \"human_label\": \"model_a\" | \"model_b\" | \"tie\" | \"both_bad\"\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Filter to examples with valid labels\n",
    "    filtered = [ex for ex in data if ex.get(\"human_label\") in label2id]\n",
    "    return filtered\n",
    "\n",
    "def split_train_val_test(data, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    # First train+temp vs test\n",
    "    train_val, test = train_test_split(\n",
    "        data, test_size=test_ratio, random_state=seed, shuffle=True\n",
    "    )\n",
    "    # Then split train vs val\n",
    "    val_size = val_ratio / (1.0 - test_ratio)\n",
    "    train, val = train_test_split(\n",
    "        train_val, test_size=val_size, random_state=seed, shuffle=True\n",
    "    )\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06ea836-8d0c-43fb-92a5-bce05da65c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 894 examples\n",
      "Train: 714 Val: 90 Test: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f490fe47a4aa9baba499e421031ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n",
      "Epoch 1 | Step 50 | Train Loss 1.4494\n",
      "Epoch 1 | Step 100 | Train Loss 1.2734\n",
      "Epoch 1 | Step 150 | Train Loss 1.1906\n",
      "Epoch 1 | Step 200 | Train Loss 1.2300\n",
      "Epoch 1 | Step 250 | Train Loss 1.3081\n",
      "Epoch 1 | Step 300 | Train Loss 1.4534\n",
      "Epoch 1 | Step 350 | Train Loss 1.4170\n",
      "Epoch 1 | Step 400 | Train Loss 1.4058\n",
      "Epoch 1 | Step 450 | Train Loss 1.3366\n",
      "Epoch 1 | Step 500 | Train Loss 1.2854\n",
      "Epoch 1 | Step 550 | Train Loss 1.2279\n",
      "Epoch 1 | Step 600 | Train Loss 1.1934\n",
      "Epoch 1 | Step 650 | Train Loss 1.1481\n",
      "Epoch 1 | Step 700 | Train Loss 1.1093\n",
      "[Epoch 1] Train Avg Loss = 1.0965\n",
      "[Epoch 1] Test Avg Loss = 0.5906 | Overall Test Acc = 0.2111\n",
      "  Per-label accuracy:\n",
      "    model_a  : 0.0000 (n=32)\n",
      "    model_b  : 0.0000 (n=21)\n",
      "    both_bad : 0.0000 (n=18)\n",
      "    tie      : 1.0000 (n=19)\n",
      "Epoch 2 | Step 50 | Train Loss 0.6183\n",
      "Epoch 2 | Step 100 | Train Loss 0.6125\n",
      "Epoch 2 | Step 150 | Train Loss 0.6134\n",
      "Epoch 2 | Step 200 | Train Loss 0.6120\n",
      "Epoch 2 | Step 250 | Train Loss 0.6022\n",
      "Epoch 2 | Step 300 | Train Loss 0.6100\n",
      "Epoch 2 | Step 350 | Train Loss 0.6266\n",
      "Epoch 2 | Step 400 | Train Loss 0.6806\n",
      "Epoch 2 | Step 450 | Train Loss 0.7048\n",
      "Epoch 2 | Step 500 | Train Loss 0.7289\n",
      "Epoch 2 | Step 550 | Train Loss 0.7175\n",
      "Epoch 2 | Step 600 | Train Loss 0.7115\n",
      "Epoch 2 | Step 650 | Train Loss 0.7034\n",
      "Epoch 2 | Step 700 | Train Loss 0.7031\n",
      "[Epoch 2] Train Avg Loss = 0.7190\n",
      "[Epoch 2] Test Avg Loss = 0.7843 | Overall Test Acc = 0.2000\n",
      "  Per-label accuracy:\n",
      "    model_a  : 0.0625 (n=32)\n",
      "    model_b  : 0.1905 (n=21)\n",
      "    both_bad : 0.6667 (n=18)\n",
      "    tie      : 0.0000 (n=19)\n",
      "Epoch 3 | Step 50 | Train Loss 0.6795\n",
      "Epoch 3 | Step 100 | Train Loss 0.6476\n",
      "Epoch 3 | Step 150 | Train Loss 0.6377\n",
      "Epoch 3 | Step 200 | Train Loss 0.6475\n",
      "Epoch 3 | Step 250 | Train Loss 0.6631\n",
      "Epoch 3 | Step 300 | Train Loss 0.6525\n",
      "Epoch 3 | Step 350 | Train Loss 0.6412\n",
      "Epoch 3 | Step 400 | Train Loss 0.6306\n",
      "Epoch 3 | Step 450 | Train Loss 0.6283\n",
      "Epoch 3 | Step 500 | Train Loss 0.6318\n",
      "Epoch 3 | Step 550 | Train Loss 0.6298\n",
      "Epoch 3 | Step 600 | Train Loss 0.6272\n",
      "Epoch 3 | Step 650 | Train Loss 0.6302\n",
      "Epoch 3 | Step 700 | Train Loss 0.6291\n",
      "[Epoch 3] Train Avg Loss = 0.6291\n",
      "[Epoch 3] Test Avg Loss = 0.6030 | Overall Test Acc = 0.2111\n",
      "  Per-label accuracy:\n",
      "    model_a  : 0.0000 (n=32)\n",
      "    model_b  : 0.0000 (n=21)\n",
      "    both_bad : 0.0000 (n=18)\n",
      "    tie      : 1.0000 (n=19)\n",
      "Saved LoRA RM to ./models/qwen2_5_math7b_reward_lora_bs_1\n"
     ]
    }
   ],
   "source": [
    "arena_data = load_arena_json(\"./data/arena_140k_math_filtered.json\")\n",
    "print(\"Loaded\", len(arena_data), \"examples\")\n",
    "\n",
    "train_data, val_data, test_data = split_train_val_test(arena_data, val_ratio=0.1, test_ratio=0.1)\n",
    "print(\"Train:\", len(train_data), \"Val:\", len(val_data), \"Test:\", len(test_data))\n",
    "# print(train_data)\n",
    "train_reward_model(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebc72170-a3e3-4503-87ae-16ed880744ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e011d27042e64b0db04b9a5e7367959f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-Math-7B\"\n",
    "LORA_DIR = \"./models/qwen2_5_math7b_reward_lora\"\n",
    "\n",
    "DELTA = 0.2   # tie margin\n",
    "TAU   = -1.0  # both_bad threshold\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=CACHE_DIR)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": DEVICE},\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "base_model = PeftModel.from_pretrained(base_model, LORA_DIR, cache_dir=CACHE_DIR)\n",
    "base_model.eval()\n",
    "\n",
    "base_dtype = next(base_model.parameters()).dtype\n",
    "\n",
    "reward_head = nn.Linear(base_model.config.hidden_size, 1)\n",
    "state_dict = torch.load(f\"{LORA_DIR}/reward_head.pt\", map_location=DEVICE)\n",
    "reward_head.load_state_dict(state_dict)\n",
    "reward_head = reward_head.to(device=DEVICE, dtype=base_dtype)\n",
    "reward_head.eval()\n",
    "\n",
    "SYSTEM_PREFIX = (\n",
    "    \"You are a strict math answer judge. \"\n",
    "    \"You will be given a question and an answer. \"\n",
    "    \"Evaluate the answer's correctness and reasoning quality.\\n\\n\"\n",
    ")\n",
    "\n",
    "def build_text(question: str, answer: str) -> str:\n",
    "    return (\n",
    "        SYSTEM_PREFIX\n",
    "        + \"Question:\\n\"\n",
    "        + question.strip()\n",
    "        + \"\\n\\nAnswer:\\n\"\n",
    "        + answer.strip()\n",
    "    )\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_single_answer(question: str, answer: str, max_len: int = 1024) -> float:\n",
    "    text = build_text(question, answer)\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    outputs = base_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    last_idx = attention_mask.sum(dim=1) - 1\n",
    "    last_hidden = hidden_states[0, last_idx.item()]  # (hidden,)\n",
    "    score = reward_head(last_hidden).squeeze().item()\n",
    "    return score\n",
    "\n",
    "def predict_pair_label(question: str, answer_a: str, answer_b: str,\n",
    "                       delta: float = DELTA, tau: float = TAU):\n",
    "    sA = score_single_answer(question, answer_a)\n",
    "    sB = score_single_answer(question, answer_b)\n",
    "    d = sA - sB\n",
    "\n",
    "    if (sA < tau) and (sB < tau):\n",
    "        label = \"both_bad\"\n",
    "    elif d > delta:\n",
    "        label = \"model_a\"\n",
    "    elif d < -delta:\n",
    "        label = \"model_b\"\n",
    "    else:\n",
    "        label = \"tie\"\n",
    "\n",
    "    return label, sA, sB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b6a7f-0bdc-451c-96e0-819bdf5b9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate_rm_on_test(test_data, delta=DELTA, tau=TAU):\n",
    "    \"\"\"\n",
    "    test_data: list of dicts with keys:\n",
    "      - \"question\"\n",
    "      - \"answer_a\"\n",
    "      - \"answer_b\"\n",
    "      - \"human_label\"\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    per_label_counts = Counter()\n",
    "    per_label_correct = Counter()\n",
    "\n",
    "    for ex in test_data:\n",
    "        q = ex[\"question\"]\n",
    "        aA = ex[\"answer_a\"]\n",
    "        aB = ex[\"answer_b\"]\n",
    "        true_label = ex[\"human_label\"]\n",
    "\n",
    "        pred_label, sA, sB = predict_pair_label(q, aA, aB, delta=delta, tau=tau)\n",
    "\n",
    "        total += 1\n",
    "        per_label_counts[true_label] += 1\n",
    "        if pred_label == true_label:\n",
    "            correct += 1\n",
    "            per_label_correct[true_label] += 1\n",
    "\n",
    "    overall_acc = correct / total if total > 0 else 0.0\n",
    "    per_label_acc = {\n",
    "        lab: (per_label_correct[lab] / cnt) if cnt > 0 else 0.0\n",
    "        for lab, cnt in per_label_counts.items()\n",
    "    }\n",
    "\n",
    "    print(f\"Test size: {total}\")\n",
    "    print(f\"Overall accuracy: {overall_acc:.4f}\")\n",
    "    print(\"Per-label accuracy:\")\n",
    "    for lab, cnt in per_label_counts.items():\n",
    "        print(f\"  {lab:9s}: {per_label_acc[lab]:.4f} (n={cnt})\")\n",
    "\n",
    "    return {\n",
    "        \"overall_acc\": overall_acc,\n",
    "        \"per_label_acc\": per_label_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1859db02-46d8-49b9-995f-5de3efebea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score_a': 1.046875, 'score_b': 0.361328125, 'diff': 0.685546875, 'label': 'model_a'}\n"
     ]
    }
   ],
   "source": [
    "# q = \"Solve the equation 3x + 5 = 17.\"\n",
    "# aA = \"3x + 5 = 17 => 3x = 12 => x = 4. So the answer is 4.\"\n",
    "# aB = \"3x = 17 + 5 = 22 so x = 22. The answer is 22.\"\n",
    "\n",
    "# result = predict_pair_label(q, aA, aB)\n",
    "# print(result)\n",
    "# e.g. {'score_a': 1.23, 'score_b': -0.45, 'diff': 1.68, 'label': 'model_a'}\n",
    "\n",
    "metrics = evaluate_rm_on_test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c4989-a583-4155-9169-4b4c8d47a0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
